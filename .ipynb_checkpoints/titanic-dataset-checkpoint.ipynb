{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1. Setup*\n",
    "#### import the modules and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.382079Z",
     "iopub.status.busy": "2023-10-13T14:54:01.381274Z",
     "iopub.status.idle": "2023-10-13T14:54:01.389079Z",
     "shell.execute_reply": "2023-10-13T14:54:01.388119Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.382045Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.393688Z",
     "iopub.status.busy": "2023-10-13T14:54:01.393179Z",
     "iopub.status.idle": "2023-10-13T14:54:01.400835Z",
     "shell.execute_reply": "2023-10-13T14:54:01.400242Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.393661Z"
    }
   },
   "outputs": [],
   "source": [
    "# ideas for this notebook:\n",
    "\n",
    "# 1. Logistic regression for classification, with tf and scikit\n",
    "# 2. Random forest for classification, with tf and scikit\n",
    "# 3. Neural Networks for classification, with tf and scikit \n",
    "\n",
    "# compare the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.404436Z",
     "iopub.status.busy": "2023-10-13T14:54:01.404212Z",
     "iopub.status.idle": "2023-10-13T14:54:01.410582Z",
     "shell.execute_reply": "2023-10-13T14:54:01.409964Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.404416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d129b0766133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# options for pandas display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# options for pandas display\n",
    "pd.options.display.float_format = \"{:.3f}\".format # show only three decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.438705Z",
     "iopub.status.busy": "2023-10-13T14:54:01.438451Z",
     "iopub.status.idle": "2023-10-13T14:54:01.461468Z",
     "shell.execute_reply": "2023-10-13T14:54:01.460703Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.438682Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "\n",
    "# shuffle the train dataset to avoid tendencies\n",
    "traind_df = train_df.reindex(np.random.permutation(train_df.index))\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.469931Z",
     "iopub.status.busy": "2023-10-13T14:54:01.469673Z",
     "iopub.status.idle": "2023-10-13T14:54:01.487722Z",
     "shell.execute_reply": "2023-10-13T14:54:01.486961Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.469907Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2. Exploratory Data Analysis (EDA)*\n",
    "#### visualize the data, found possible errors in it, missing values, looks for hints for the better features, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.489177Z",
     "iopub.status.busy": "2023-10-13T14:54:01.488927Z",
     "iopub.status.idle": "2023-10-13T14:54:01.504558Z",
     "shell.execute_reply": "2023-10-13T14:54:01.503607Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.489155Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "\n",
    "# concatenate train and test sets\n",
    "df = pd.concat([train_df, test_df], axis=0) \n",
    "df\n",
    "\n",
    "# some visualization of the dataset\n",
    "# print(\"df info:\")\n",
    "# print()\n",
    "# df.info()\n",
    "# print()\n",
    "\n",
    "print(\"null values count:\")\n",
    "print()\n",
    "df.isnull().sum()\n",
    "\n",
    "# analysis\n",
    "# so we have missing values for:\n",
    "\n",
    "# Age (263, about 20%), can be a good feature, we will need to treat this missing values.\n",
    "# Fare (only 1), we will fill this data and will have no problem, it is only one.\n",
    "# Cabin (most of the values, more than 75%), difficult to fill with veracity when more than 75% of the values are missing.\n",
    "# Embarked (only 2), we will fill this data and will have no problem, it is only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.510218Z",
     "iopub.status.busy": "2023-10-13T14:54:01.509337Z",
     "iopub.status.idle": "2023-10-13T14:54:01.547647Z",
     "shell.execute_reply": "2023-10-13T14:54:01.546595Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.510192Z"
    }
   },
   "outputs": [],
   "source": [
    "# list with the names of numerical and categorical columns\n",
    "numerical_columns = ['PassengerId', 'Survived', 'Pclass', 'Fare', 'Age', 'SibSp', 'Parch']\n",
    "categorical_columns = [x for x in df.columns if x not in numerical_columns]  \n",
    "\n",
    "# print the .describe() with all de column types\n",
    "print(\"data describe:\")\n",
    "print()\n",
    "df[numerical_columns + categorical_columns].describe(include=\"all\").T\n",
    "\n",
    "# analysis\n",
    "# 1. 38,4% of the people survived (not very imbalanced) (the .describe() doesn't count the NaN's when calculating the average)\n",
    "# 2. 843 males vs 466 female (a little imbalanced)\n",
    "# 3. SibSp and Parch max of 8 and 9 respectively, which is kind of ok considering the average number of childrens were greater:\n",
    "# https://populationeducation.org/wp-content/uploads/2020/04/average-number-children-per-us-family-historic-infographic.pdf\n",
    "# but most people had no (or few) relatives on board, we can see this by the quantiles\n",
    "# 4.186 unique cabins (of only 295 values, can't categorize by this feature)\n",
    "\n",
    "# questions\n",
    "# 1. minimum fare of 0 (15 values)? no explanation for this in the dataset description\n",
    "# 2. the tickets aren't unique for each person, the same family members get the same ticket? no explanation for this in the dataset description\n",
    "# not really, sometimes yes sometimes not, better do this by the name of the family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:01.549184Z",
     "iopub.status.busy": "2023-10-13T14:54:01.548963Z",
     "iopub.status.idle": "2023-10-13T14:54:10.535254Z",
     "shell.execute_reply": "2023-10-13T14:54:10.534349Z",
     "shell.execute_reply.started": "2023-10-13T14:54:01.549164Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of distributions\n",
    "# here we use only the train dataframe because we need the Survived values\n",
    "plt.figure()\n",
    "sns.pairplot(train_df, hue=\"Survived\")\n",
    "\n",
    "# analysis\n",
    "# 1. most people from the Pclass=3 died than lived, in Pclass=1,2 the number is alike\n",
    "# 2. high (4 or more) SibSp or Parch seems to have died more \n",
    "\n",
    "# 3. none of the other distributions seem to indicate a clear line between the people that survived and those who didn't\n",
    "# in other words, do not show trends of people who survived in function of the features (or a relantionship between features and survived) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:10.537057Z",
     "iopub.status.busy": "2023-10-13T14:54:10.536565Z",
     "iopub.status.idle": "2023-10-13T14:54:10.974766Z",
     "shell.execute_reply": "2023-10-13T14:54:10.973764Z",
     "shell.execute_reply.started": "2023-10-13T14:54:10.537025Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr().abs(), cmap=\"Purples\", annot=True)\n",
    "\n",
    "# Analysis\n",
    "# we dont have much correlation in the dataset\n",
    "# the most useful one is the 0.34 in Survived x Pclass and it's not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:10.977050Z",
     "iopub.status.busy": "2023-10-13T14:54:10.976671Z",
     "iopub.status.idle": "2023-10-13T14:54:10.980665Z",
     "shell.execute_reply": "2023-10-13T14:54:10.979851Z",
     "shell.execute_reply.started": "2023-10-13T14:54:10.977025Z"
    }
   },
   "outputs": [],
   "source": [
    "# try this later \n",
    "# !pip install dython\n",
    "# from dython import nominal\n",
    "# nominal.associations(df,figsize=(20,10),mark_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:10.982343Z",
     "iopub.status.busy": "2023-10-13T14:54:10.981975Z",
     "iopub.status.idle": "2023-10-13T14:54:11.015683Z",
     "shell.execute_reply": "2023-10-13T14:54:11.014748Z",
     "shell.execute_reply.started": "2023-10-13T14:54:10.982294Z"
    }
   },
   "outputs": [],
   "source": [
    "# to show all the dataframes\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "\n",
    "# check if the data are imbalanced\n",
    "\n",
    "# male and female who survived\n",
    "df_temp1 = train_df.groupby(['Sex','Survived'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "# most female survived while most men died, good feature\n",
    "\n",
    "# Pclass and their survival\n",
    "df_temp2 = train_df.groupby(['Pclass','Survived'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "# the number is kind of equal except for the 3 Pclass where more people died and a little for the 1 Pclass\n",
    "\n",
    "# SibSp and their survival\n",
    "df_temp3 = train_df.groupby(['SibSp','Survived'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "# not much difference of counts in the same values of SibSp, except for zero, where more people died\n",
    "\n",
    "# Parch and their survival\n",
    "df_temp4 = train_df.groupby(['Parch','Survived'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "# in Parch=0 more people die (2/3) for others Parch's there is not much difference\n",
    "\n",
    "# SibSp + Parch and their survival\n",
    "df_temp4 = train_df.groupby(['Parch','Survived'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "# in Parch=0 more people die (2/3) for others Parch's there is not much difference\n",
    "\n",
    "display_side_by_side(df_temp1,df_temp2,df_temp3,df_temp4, titles=['Sex-Survived','Pclass-Survived',\"SibSp-Survived\",\"Parch-Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data Preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.017930Z",
     "iopub.status.busy": "2023-10-13T14:54:11.017076Z",
     "iopub.status.idle": "2023-10-13T14:54:11.237936Z",
     "shell.execute_reply": "2023-10-13T14:54:11.237222Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.017894Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if age has a correlation with sex\n",
    "\n",
    "# group by sex and age\n",
    "df_temp = train_df.groupby(['Sex','Age'],as_index=False).size().rename(columns={\"size\":'counts'})\n",
    "\n",
    "# histogram of age divided by sex\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_temp[df_temp[\"Sex\"]==\"female\"][\"Age\"], weights=df_temp[df_temp[\"Sex\"]==\"female\"][\"counts\"] )\n",
    "plt.title(\"Female\")\n",
    "plt.xlim(0,80)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_temp[df_temp[\"Sex\"]==\"male\"][\"Age\"], weights=df_temp[df_temp[\"Sex\"]==\"male\"][\"counts\"] )\n",
    "plt.title(\"Male\")\n",
    "plt.xlim(0,80)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.239350Z",
     "iopub.status.busy": "2023-10-13T14:54:11.238966Z",
     "iopub.status.idle": "2023-10-13T14:54:11.262936Z",
     "shell.execute_reply": "2023-10-13T14:54:11.262162Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.239325Z"
    }
   },
   "outputs": [],
   "source": [
    "# treat the data\n",
    "\n",
    "# lets start by filling the NaN's in age \n",
    "# Pclass has a correlation of 0.41 with age, sex seems to have low correlation, we will group by Pclass\n",
    "# creates a dataframe grouped by Pclass with the mean of Age for each Pclass (contains columns Mean Age, Pclass and others)\n",
    "df_temp = df.groupby(\"Pclass\", as_index=False).mean().rename(columns={\"Age\":\"Mean Age\"})\n",
    "\n",
    "# creates a dictionary from Pclass to medium age\n",
    "dict_temp = dict(zip(df_temp[\"Pclass\"], df_temp[\"Mean Age\"]))\n",
    "\n",
    "# fill the nan's mapping the dictionary \n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Pclass\"].map(dict_temp))\n",
    "\n",
    "\n",
    "\n",
    "# then normalize the fare column by the Z-score\n",
    "# df[\"Fare\"] = ( df[\"Fare\"]-df[\"Fare\"].mean() ) / df[\"Fare\"].std() \n",
    "\n",
    "# then normalize the fare column between 0 and 1\n",
    "df[\"Fare\"] = df[\"Fare\"] / df[\"Fare\"].max()\n",
    "\n",
    "\n",
    "# creates a new column in the dataframe with the family size \n",
    "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.264387Z",
     "iopub.status.busy": "2023-10-13T14:54:11.264162Z",
     "iopub.status.idle": "2023-10-13T14:54:11.269907Z",
     "shell.execute_reply": "2023-10-13T14:54:11.268985Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.264367Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the data back in train and test sets after treated\n",
    "train_df_treated = df.iloc[:890]\n",
    "test_df_treated = df.iloc[891:].drop([\"Survived\"], axis=1)\n",
    "# all values of \"Survived\" were NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3. Building and comparing three diferent ML models*\n",
    "### 3.1 Logistic Regression - TensorFlow\n",
    "### 3.2 Random Forests - Scikit-learn\n",
    "### 3.3 Gradient Boosted Trees - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3.1 Logistic Regression - TensorFlow*\n",
    "\n",
    "#### heavily based on the Google ML Crash Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.272055Z",
     "iopub.status.busy": "2023-10-13T14:54:11.271229Z",
     "iopub.status.idle": "2023-10-13T14:54:11.285062Z",
     "shell.execute_reply": "2023-10-13T14:54:11.284048Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.272029Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates the feature layer\n",
    "\n",
    "# list with the name of the numerical features\n",
    "list_numerical_features = [\"Fare\"]\n",
    "\n",
    "# list with the name of the bucketized features\n",
    "list_bucketized_features = [\"Age\", \"FamilySize\"]\n",
    "# dictionary with resolution of intervals of each bucketized feature\n",
    "dict_bucketized_features = {\"Age\":10, \"FamilySize\":2}\n",
    "\n",
    "# list with categorical features  \n",
    "list_categorical_features = [\"Sex\", \"Pclass\"]\n",
    "# dictionary with vocabulary of each categorical feature\n",
    "dict_categorical_features = {\"Sex\":[\"male\",\"female\"], \"Pclass\":[1,2,3]}\n",
    "\n",
    "\n",
    "\n",
    "# join all the features\n",
    "list_features = []\n",
    "list_features.extend(list_numerical_features[:])\n",
    "list_features.extend(list_bucketized_features[:])\n",
    "list_features.extend(list_categorical_features[:])\n",
    "\n",
    "# list with the feature columns\n",
    "feature_columns = []\n",
    "\n",
    "\n",
    "# creates the feature columns\n",
    "\n",
    "# create numerical feature columns\n",
    "for name in list_numerical_features:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(name))\n",
    "    \n",
    "# create bucketized feature columns\n",
    "for name in list_bucketized_features:\n",
    "    numeric = tf.feature_column.numeric_column(name)\n",
    "    boundaries = list(np.arange( int(min(df[name])), int(max(df[name])), dict_bucketized_features[name] ))\n",
    "    bucketized_feature = tf.feature_column.bucketized_column(numeric, boundaries=boundaries)\n",
    "    feature_columns.append(bucketized_feature)\n",
    "    \n",
    "# create categorical feature columns\n",
    "for name in list_categorical_features:    \n",
    "    categorical_feature = tf.feature_column.categorical_column_with_vocabulary_list(key=name, vocabulary_list=dict_categorical_features[name], default_value=0)\n",
    "    embedded_group_column = tf.feature_column.embedding_column(categorical_feature,dimension=len(dict_categorical_features[name]))\n",
    "    feature_columns.append(embedded_group_column)\n",
    "    \n",
    "\n",
    "# shows the feature columns to see if everything is allright\n",
    "print(feature_columns)\n",
    "    \n",
    "# finally\n",
    "# convert the features into a layer\n",
    "feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.289704Z",
     "iopub.status.busy": "2023-10-13T14:54:11.288915Z",
     "iopub.status.idle": "2023-10-13T14:54:11.302421Z",
     "shell.execute_reply": "2023-10-13T14:54:11.301716Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.289664Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions that create and train a model\n",
    "\n",
    "# classification model\n",
    "def Create_Model(learning_rate, feature_layer, metrics):\n",
    "    \n",
    "    # sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # add the feature layer we created\n",
    "    model.add(feature_layer)\n",
    "    \n",
    "    # pass the regression value trough a sigmoid activation\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,), activation=tf.sigmoid))\n",
    "    \n",
    "    # compile into a model that tensorflow can execute with a Binary Cross Entropy loss function\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "def Train_Model(model, dataset, epochs, label_name, batch_size=None, shuffle=True):\n",
    "    \n",
    "    # create dict with name of the features and numpy arrays of its values for each feature\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    \n",
    "    # removes the label from features and puts it into variable label\n",
    "    label = np.array(features.pop(label_name))\n",
    "    \n",
    "    # trains the model for a fixed number of epochs\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size, epochs=epochs, shuffle=shuffle, verbose=1)\n",
    "    \n",
    "    # the list of epochs\n",
    "    epochs_list = history.epoch\n",
    "    \n",
    "    # classification metrics (training loss values and metric values [and validation if applicable]) for each epoch\n",
    "    classification_metrics_history = pd.DataFrame(history.history)\n",
    "    \n",
    "    return epochs_list, classification_metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.304171Z",
     "iopub.status.busy": "2023-10-13T14:54:11.303588Z",
     "iopub.status.idle": "2023-10-13T14:54:11.317493Z",
     "shell.execute_reply": "2023-10-13T14:54:11.316865Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.304137Z"
    }
   },
   "outputs": [],
   "source": [
    "# defines the plotting function \n",
    "\n",
    "def Plot_Curve(epochs_list, classification_metrics_history, list_of_metrics):\n",
    "    #plot the curves of one or more classification metrics vs epoch\n",
    "    # list_of_metrics should be one of the names shown in:\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    \n",
    "    for metric in list_of_metrics:\n",
    "        x = classification_metrics_history[metric]\n",
    "        plt.plot(epochs_list[1:], x[1:], label=metric)\n",
    "        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:11.318728Z",
     "iopub.status.busy": "2023-10-13T14:54:11.318305Z",
     "iopub.status.idle": "2023-10-13T14:54:15.535965Z",
     "shell.execute_reply": "2023-10-13T14:54:15.535007Z",
     "shell.execute_reply.started": "2023-10-13T14:54:11.318704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the hyperparameters, train the model and plot the result of training\n",
    "\n",
    "# hyperparameters \n",
    "\n",
    "# output values above the classification_threshold will be considered as survived (1), below as dead (0)\n",
    "classification_threshold = 0.65\n",
    "# the multiplication factor of the gradient update\n",
    "learning_rate = 0.005\n",
    "# how many iterations over the whole datset\n",
    "epochs = 100\n",
    "# number of iterations (values) before gradient calculation and update\n",
    "batch_size = 100\n",
    "\n",
    "# creates the label name\n",
    "label_name = \"Survived\"\n",
    "\n",
    "# the metrics the model will measure\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=classification_threshold),\n",
    "           tf.keras.metrics.Precision(name=\"precision\", thresholds=classification_threshold),\n",
    "           tf.keras.metrics.Recall(name=\"recall\", thresholds=classification_threshold),\n",
    "           tf.keras.metrics.AUC(name=\"auc\", num_thresholds=100),\n",
    "          ]\n",
    "\n",
    "# creates the model \n",
    "model = Create_Model(learning_rate, feature_layer, metrics)\n",
    "\n",
    "# dataframe with only the columns we will use\n",
    "df_temp = pd.concat([train_df_treated[list_features], train_df_treated[label_name]], axis=1)\n",
    "\n",
    "# train the model on the training set\n",
    "epochs_list, classification_metrics_history = Train_Model(model, df_temp, epochs, label_name, batch_size)\n",
    "\n",
    "# plot the graphs of the chosen metrics vs epochs \n",
    "list_of_metrics_to_plot = [\"accuracy\"]\n",
    "Plot_Curve(epochs_list, classification_metrics_history, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:15.538218Z",
     "iopub.status.busy": "2023-10-13T14:54:15.537270Z",
     "iopub.status.idle": "2023-10-13T14:54:15.835778Z",
     "shell.execute_reply": "2023-10-13T14:54:15.834711Z",
     "shell.execute_reply.started": "2023-10-13T14:54:15.538179Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the model against the test set\n",
    "\n",
    "# create a dict with name of the features and numpy arrays of its values - of the test set\n",
    "features = {name:np.array(value) for name, value in test_df_treated[list_features].items()}\n",
    "\n",
    "# if you have a test set with label:\n",
    "# separates the label\n",
    "# label = np.array(features.pop(label_name))\n",
    "# evaluate the model\n",
    "# model.evaluate(x=features, y=label, batch_size=batch_size)\n",
    "\n",
    "# if you have a test set without label (our case) and has to make predictions:\n",
    "predictions = model.predict(x=features, batch_size=batch_size) # numpy array\n",
    "\n",
    "# transform into 0's and 1's according to the classification threshold  \n",
    "predictions = (predictions > classification_threshold).astype(int)\n",
    "\n",
    "# check if there are any problems (Nan's)\n",
    "if np.sum(np.isnan(predictions))==0:\n",
    "    print(\"Zero NaN's\")\n",
    "else:\n",
    "    print(\"There are {} NaN's\".format(np.sum(np.isnan(predictions))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:15.837422Z",
     "iopub.status.busy": "2023-10-13T14:54:15.837058Z",
     "iopub.status.idle": "2023-10-13T14:54:15.850013Z",
     "shell.execute_reply": "2023-10-13T14:54:15.848873Z",
     "shell.execute_reply.started": "2023-10-13T14:54:15.837385Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates the submission file\n",
    "\n",
    "# gets the PassengerId column of the test data\n",
    "passengerid_df = pd.DataFrame(test_df_treated[\"PassengerId\"], columns=[\"PassengerId\"])\n",
    "# transforms predicions into dataframe\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"Survived\"])\n",
    "\n",
    "# join in one dataframe with two columns\n",
    "submission_df = pd.concat([passengerid_df, predictions_df], axis=1)\n",
    "\n",
    "# checks again if everything is alright\n",
    "print(submission_df)\n",
    "\n",
    "# save the predictions into a csv file\n",
    "submission_df.to_csv(\"TitanicPredictions_LogisticRegression_TF.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4.2 Random Forest - Scikit-learn*\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:15.851232Z",
     "iopub.status.busy": "2023-10-13T14:54:15.850988Z",
     "iopub.status.idle": "2023-10-13T14:54:15.862130Z",
     "shell.execute_reply": "2023-10-13T14:54:15.861117Z",
     "shell.execute_reply.started": "2023-10-13T14:54:15.851211Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_treated.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:15.863613Z",
     "iopub.status.busy": "2023-10-13T14:54:15.863353Z",
     "iopub.status.idle": "2023-10-13T14:54:16.755352Z",
     "shell.execute_reply": "2023-10-13T14:54:16.754651Z",
     "shell.execute_reply.started": "2023-10-13T14:54:15.863589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the dataset into a TensorFlow dataset.\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df_treated, label=\"Survived\")\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df_treated)\n",
    "\n",
    "# Train a Random Forest model.\n",
    "model_rf = tfdf.keras.RandomForestModel()\n",
    "fitting = model_rf.fit(train_ds)\n",
    "\n",
    "# Summary of the model structure.\n",
    "# summary_rf = model_rf.summary()\n",
    "\n",
    "# Evaluate the model.\n",
    "#evaluation_rf = model_rf.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "#tfdf.model_plotter.plot_model_in_colab(model_rf, tree_idx=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:16.757055Z",
     "iopub.status.busy": "2023-10-13T14:54:16.756591Z",
     "iopub.status.idle": "2023-10-13T14:54:16.953774Z",
     "shell.execute_reply": "2023-10-13T14:54:16.952889Z",
     "shell.execute_reply.started": "2023-10-13T14:54:16.757018Z"
    }
   },
   "outputs": [],
   "source": [
    "logs = model_rf.make_inspector().training_logs()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Logloss (out-of-bag)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:16.955760Z",
     "iopub.status.busy": "2023-10-13T14:54:16.955032Z",
     "iopub.status.idle": "2023-10-13T14:54:17.076284Z",
     "shell.execute_reply": "2023-10-13T14:54:17.075508Z",
     "shell.execute_reply.started": "2023-10-13T14:54:16.955721Z"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions about the test set of Titanic wich has no labels:\n",
    "\n",
    "# if you have a test set without label (our case) and has to make predictions:\n",
    "predictions_rf = model_rf.predict(test_ds) # numpy array\n",
    "\n",
    "# transform into 0's and 1's according to the classification threshold  \n",
    "predictions_rf = (predictions_rf > classification_threshold).astype(int)\n",
    "\n",
    "# check if there are any problems (Nan's)\n",
    "if np.sum(np.isnan(predictions_rf))==0:\n",
    "    print(\"Zero NaN's\")\n",
    "else:\n",
    "    print(\"There are {} NaN's\".format(np.sum(np.isnan(predictions_rf))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:54:17.077835Z",
     "iopub.status.busy": "2023-10-13T14:54:17.077362Z",
     "iopub.status.idle": "2023-10-13T14:54:17.088639Z",
     "shell.execute_reply": "2023-10-13T14:54:17.087579Z",
     "shell.execute_reply.started": "2023-10-13T14:54:17.077792Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates the submission file\n",
    "\n",
    "# gets the PassengerId column of the test data\n",
    "passengerid_df = pd.DataFrame(test_df_treated[\"PassengerId\"], columns=[\"PassengerId\"])\n",
    "# transforms predicions into dataframe\n",
    "predictions_df = pd.DataFrame(predictions_rf, columns=[\"Survived\"])\n",
    "\n",
    "# join in one dataframe with two columns\n",
    "submission_df = pd.concat([passengerid_df, predictions_df], axis=1)\n",
    "\n",
    "# checks again if everything is alright\n",
    "print(submission_df)\n",
    "\n",
    "# save the predictions into a csv file\n",
    "submission_df.to_csv(\"TitanicPredictions_RandomForest_TF.csv\", index=False, sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
